{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n",
      "<ipython-input-1-1f1e2eabb756>:73: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner import HyperModel\n"
     ]
    }
   ],
   "source": [
    "# library\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as kl\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
    "from keras.layers import BatchNormalization, InputLayer, Input\n",
    "from keras import models\n",
    "from keras.models import Sequential, Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "import IOHelper\n",
    "import SequenceHelper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('Neural_Network_DNA_Demo/')\n",
    "import random\n",
    "random.seed(1234)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from eugene.models.base import BaseModel\n",
    "# Load in relevant libraries, and alias where appropriate\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms \n",
    "from torch.utils import data\n",
    "import seaborn as sns\n",
    "import math as math\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import spearmanr\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.figure_factory as ff\n",
    "import upsetplot\n",
    "import umap\n",
    "from Bio import SeqIO\n",
    "from Levenshtein import distance\n",
    "from matplotlib_venn import venn3\n",
    "import re\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "pd.set_option('max_colwidth',260)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import plotly.subplots as sp\n",
    "from sklearn.metrics import r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch, BayesianOptimization, Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3525, 248, 4)\n",
      "Train\n",
      "(1512, 248, 4)\n",
      "Val\n",
      "(482, 248, 4)\n",
      "Test\n"
     ]
    }
   ],
   "source": [
    "# function to load sequences and enhancer activity\n",
    "def prepare_input(set):\n",
    "    # Convert sequences to one-hot encoding matrix\n",
    "    file_seq = str(\"Seq_\" + set + \".fa\")\n",
    "    input_fasta_data_A = IOHelper.get_fastas_from_file(file_seq, uppercase=True)\n",
    "\n",
    "    # get length of first sequence\n",
    "    sequence_length = len(input_fasta_data_A.sequence.iloc[0])\n",
    "\n",
    "    # Convert sequence to one hot encoding matrix\n",
    "    seq_matrix_A = SequenceHelper.do_one_hot_encoding(input_fasta_data_A.sequence, sequence_length,\n",
    "                                                      SequenceHelper.parse_alpha_to_seq)\n",
    "    print(seq_matrix_A.shape)\n",
    "    \n",
    "    X = np.nan_to_num(seq_matrix_A) # Replace NaN with zero and infinity with large finite numbers\n",
    "    X_reshaped = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
    "\n",
    "    Activity = pd.read_table(\"Seq_activity_\" + set + \".txt\")\n",
    "    Y_dev = Activity.Dev_log2_enrichment\n",
    "    Y_hk = Activity.Hk_log2_enrichment\n",
    "    Y = [Y_dev, Y_hk]\n",
    "    \n",
    "    print(set)\n",
    "\n",
    "    return input_fasta_data_A.sequence, seq_matrix_A, X_reshaped, Y\n",
    "\n",
    "# Data for train/val/test sets\n",
    "X_train_sequence, X_train_seq_matrix, X_train, Y_train = prepare_input(\"Train\")\n",
    "X_valid_sequence, X_valid_seq_matrix, X_valid, Y_valid = prepare_input(\"Val\")\n",
    "X_test_sequence, X_test_seq_matrix, X_test, Y_test = prepare_input(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Additional metrics\n",
    "from scipy.stats import spearmanr\n",
    "def Spearman(y_true, y_pred):\n",
    "     return ( tf.py_function(spearmanr, [tf.cast(y_pred, tf.float32), \n",
    "                       tf.cast(y_true, tf.float32)], Tout = tf.float32) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 248, 4)]     0           []                               \n",
      "                                                                                                  \n",
      " Conv1D_1st (Conv1D)            (None, 248, 256)     7424        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 248, 256)    1024        ['Conv1D_1st[0][0]']             \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 248, 256)     0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 124, 256)     0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " Conv1D_2 (Conv1D)              (None, 124, 60)      46140       ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 124, 60)     240         ['Conv1D_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 124, 60)      0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 62, 60)      0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " Conv1D_3 (Conv1D)              (None, 62, 60)       18060       ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 62, 60)      240         ['Conv1D_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 62, 60)       0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 31, 60)      0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " Conv1D_4 (Conv1D)              (None, 31, 120)      21720       ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 31, 120)     480         ['Conv1D_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 31, 120)      0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 15, 120)     0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1800)         0           ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " Dense_1 (Dense)                (None, 256)          461056      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 256)         1024        ['Dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 256)          0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " Dense_2 (Dense)                (None, 256)          65792       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 256)         1024        ['Dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 256)          0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 256)          0           ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " Dense_A (Dense)                (None, 1)            257         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " Dense_B (Dense)                (None, 1)            257         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 624,738\n",
      "Trainable params: 622,722\n",
      "Non-trainable params: 2,016\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'epochs': 100,\n",
       " 'early_stop': 10,\n",
       " 'kernel_size1': 7,\n",
       " 'kernel_size2': 3,\n",
       " 'kernel_size3': 5,\n",
       " 'kernel_size4': 3,\n",
       " 'learning_rate': 0.001,\n",
       " 'num_filters': 256,\n",
       " 'num_filters2': 60,\n",
       " 'num_filters3': 60,\n",
       " 'num_filters4': 120,\n",
       " 'n_conv_layer': 4,\n",
       " 'n_add_layer': 2,\n",
       " 'dropout_prob': 0.4,\n",
       " 'dense_neurons1': 256,\n",
       " 'dense_neurons2': 256,\n",
       " 'pad': 'same'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "params = {'batch_size': 128,\n",
    "          'epochs': 100,\n",
    "          'early_stop': 10,\n",
    "          'kernel_size1': 7,\n",
    "          'kernel_size2': 3,\n",
    "          'kernel_size3': 5,\n",
    "          'kernel_size4': 3,\n",
    "          'learning_rate': 0.001,\n",
    "          'num_filters': 256,\n",
    "          'num_filters2': 60,\n",
    "          'num_filters3': 60,\n",
    "          'num_filters4': 120,\n",
    "          'n_conv_layer': 4,\n",
    "          'n_add_layer': 2,\n",
    "          'dropout_prob': 0.4,\n",
    "          'dense_neurons1': 256,\n",
    "          'dense_neurons2': 256,\n",
    "          'pad':'same'}\n",
    "\n",
    "### Additional metrics\n",
    "from scipy.stats import spearmanr\n",
    "def Spearman(y_true, y_pred):\n",
    "     return ( tf.py_function(spearmanr, [tf.cast(y_pred, tf.float32), \n",
    "                       tf.cast(y_true, tf.float32)], Tout = tf.float32) )\n",
    "def SynIgP(params=params):\n",
    "    \n",
    "    learning_rate = params['learning_rate']\n",
    "    dropout_prob = params['dropout_prob']\n",
    "    n_conv_layer = params['n_conv_layer']\n",
    "    n_add_layer = params['n_add_layer']\n",
    "    \n",
    "    # body\n",
    "    input = kl.Input(shape=(248, 4))\n",
    "    x = kl.Conv1D(params['num_filters'], kernel_size=params['kernel_size1'],\n",
    "                  padding=params['pad'],\n",
    "                  name='Conv1D_1st')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    for i in range(1, n_conv_layer):\n",
    "        x = kl.Conv1D(params['num_filters'+str(i+1)],\n",
    "                      kernel_size=params['kernel_size'+str(i+1)],\n",
    "                      padding=params['pad'],\n",
    "                      name=str('Conv1D_'+str(i+1)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # dense layers\n",
    "    for i in range(0, n_add_layer):\n",
    "        x = kl.Dense(params['dense_neurons'+str(i+1)],\n",
    "                     name=str('Dense_'+str(i+1)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "    bottleneck = x\n",
    "    \n",
    "    # heads per task (rep1 and rep2)\n",
    "    tasks = ['A', 'B']\n",
    "    outputs = []\n",
    "    for task in tasks:\n",
    "        outputs.append(kl.Dense(1, activation='linear', name=str('Dense_' + task))(bottleneck))\n",
    "\n",
    "    model = keras.models.Model([input], outputs)\n",
    "    model.compile(keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss=['mse', 'mse'], # loss\n",
    "                  loss_weights=[1, 1], # loss weigths to balance\n",
    "                  metrics=[Spearman]) # additional track metric\n",
    "\n",
    "    return model, params\n",
    "\n",
    "SynIgP()[0].summary()\n",
    "SynIgP()[1] # dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "28/28 [==============================] - 15s 445ms/step - loss: 7.9561 - Dense_A_loss: 3.7106 - Dense_B_loss: 4.2454 - Dense_A_Spearman: 0.4603 - Dense_B_Spearman: 0.4396 - val_loss: 9.9538 - val_Dense_A_loss: 4.8922 - val_Dense_B_loss: 5.0616 - val_Dense_A_Spearman: 0.5136 - val_Dense_B_Spearman: 0.5129\n",
      "Epoch 2/100\n",
      "28/28 [==============================] - 12s 427ms/step - loss: 5.6483 - Dense_A_loss: 2.7271 - Dense_B_loss: 2.9212 - Dense_A_Spearman: 0.5855 - Dense_B_Spearman: 0.5665 - val_loss: 9.9125 - val_Dense_A_loss: 4.5813 - val_Dense_B_loss: 5.3313 - val_Dense_A_Spearman: 0.5359 - val_Dense_B_Spearman: 0.5380\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - 12s 432ms/step - loss: 5.1885 - Dense_A_loss: 2.5468 - Dense_B_loss: 2.6418 - Dense_A_Spearman: 0.6206 - Dense_B_Spearman: 0.6118 - val_loss: 9.3640 - val_Dense_A_loss: 4.2560 - val_Dense_B_loss: 5.1080 - val_Dense_A_Spearman: 0.5232 - val_Dense_B_Spearman: 0.5023\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - 12s 436ms/step - loss: 4.4313 - Dense_A_loss: 2.1269 - Dense_B_loss: 2.3044 - Dense_A_Spearman: 0.6767 - Dense_B_Spearman: 0.6485 - val_loss: 9.3973 - val_Dense_A_loss: 4.1459 - val_Dense_B_loss: 5.2514 - val_Dense_A_Spearman: 0.4912 - val_Dense_B_Spearman: 0.4827\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - 12s 444ms/step - loss: 3.8241 - Dense_A_loss: 1.8881 - Dense_B_loss: 1.9360 - Dense_A_Spearman: 0.7173 - Dense_B_Spearman: 0.7066 - val_loss: 9.2871 - val_Dense_A_loss: 4.1347 - val_Dense_B_loss: 5.1523 - val_Dense_A_Spearman: 0.4730 - val_Dense_B_Spearman: 0.5006\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - 12s 436ms/step - loss: 3.3116 - Dense_A_loss: 1.6119 - Dense_B_loss: 1.6997 - Dense_A_Spearman: 0.7617 - Dense_B_Spearman: 0.7457 - val_loss: 9.2804 - val_Dense_A_loss: 4.1396 - val_Dense_B_loss: 5.1408 - val_Dense_A_Spearman: 0.3953 - val_Dense_B_Spearman: 0.4550\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - 12s 435ms/step - loss: 2.9970 - Dense_A_loss: 1.4626 - Dense_B_loss: 1.5344 - Dense_A_Spearman: 0.7876 - Dense_B_Spearman: 0.7716 - val_loss: 9.9556 - val_Dense_A_loss: 4.4206 - val_Dense_B_loss: 5.5351 - val_Dense_A_Spearman: 0.5097 - val_Dense_B_Spearman: 0.5625\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - 12s 441ms/step - loss: 2.6904 - Dense_A_loss: 1.3012 - Dense_B_loss: 1.3892 - Dense_A_Spearman: 0.8158 - Dense_B_Spearman: 0.8032 - val_loss: 10.9548 - val_Dense_A_loss: 4.7089 - val_Dense_B_loss: 6.2459 - val_Dense_A_Spearman: 0.4890 - val_Dense_B_Spearman: 0.5226\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - 13s 456ms/step - loss: 2.2668 - Dense_A_loss: 1.0795 - Dense_B_loss: 1.1873 - Dense_A_Spearman: 0.8438 - Dense_B_Spearman: 0.8256 - val_loss: 9.8471 - val_Dense_A_loss: 4.2215 - val_Dense_B_loss: 5.6255 - val_Dense_A_Spearman: 0.5033 - val_Dense_B_Spearman: 0.5317\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - 12s 429ms/step - loss: 2.1389 - Dense_A_loss: 1.0251 - Dense_B_loss: 1.1138 - Dense_A_Spearman: 0.8597 - Dense_B_Spearman: 0.8463 - val_loss: 9.2084 - val_Dense_A_loss: 3.9449 - val_Dense_B_loss: 5.2635 - val_Dense_A_Spearman: 0.5351 - val_Dense_B_Spearman: 0.5613\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - 12s 426ms/step - loss: 1.8855 - Dense_A_loss: 0.9217 - Dense_B_loss: 0.9638 - Dense_A_Spearman: 0.8740 - Dense_B_Spearman: 0.8647 - val_loss: 9.7161 - val_Dense_A_loss: 4.2351 - val_Dense_B_loss: 5.4809 - val_Dense_A_Spearman: 0.5572 - val_Dense_B_Spearman: 0.5660\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - 12s 422ms/step - loss: 1.7078 - Dense_A_loss: 0.8496 - Dense_B_loss: 0.8582 - Dense_A_Spearman: 0.8816 - Dense_B_Spearman: 0.8787 - val_loss: 9.6914 - val_Dense_A_loss: 4.2536 - val_Dense_B_loss: 5.4377 - val_Dense_A_Spearman: 0.5570 - val_Dense_B_Spearman: 0.5590\n",
      "Epoch 13/100\n",
      "28/28 [==============================] - 12s 423ms/step - loss: 1.6371 - Dense_A_loss: 0.7851 - Dense_B_loss: 0.8520 - Dense_A_Spearman: 0.8920 - Dense_B_Spearman: 0.8806 - val_loss: 9.3634 - val_Dense_A_loss: 4.1460 - val_Dense_B_loss: 5.2173 - val_Dense_A_Spearman: 0.5725 - val_Dense_B_Spearman: 0.5818\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - 12s 437ms/step - loss: 1.5248 - Dense_A_loss: 0.7408 - Dense_B_loss: 0.7840 - Dense_A_Spearman: 0.8982 - Dense_B_Spearman: 0.8944 - val_loss: 8.3407 - val_Dense_A_loss: 3.7686 - val_Dense_B_loss: 4.5721 - val_Dense_A_Spearman: 0.5782 - val_Dense_B_Spearman: 0.5644\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - 12s 437ms/step - loss: 1.4747 - Dense_A_loss: 0.7060 - Dense_B_loss: 0.7687 - Dense_A_Spearman: 0.9045 - Dense_B_Spearman: 0.8928 - val_loss: 7.6019 - val_Dense_A_loss: 3.4579 - val_Dense_B_loss: 4.1439 - val_Dense_A_Spearman: 0.5814 - val_Dense_B_Spearman: 0.5775\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - 12s 422ms/step - loss: 1.3189 - Dense_A_loss: 0.6224 - Dense_B_loss: 0.6965 - Dense_A_Spearman: 0.9147 - Dense_B_Spearman: 0.9045 - val_loss: 7.6465 - val_Dense_A_loss: 3.5449 - val_Dense_B_loss: 4.1016 - val_Dense_A_Spearman: 0.5773 - val_Dense_B_Spearman: 0.5789\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - 12s 424ms/step - loss: 1.2204 - Dense_A_loss: 0.6108 - Dense_B_loss: 0.6096 - Dense_A_Spearman: 0.9158 - Dense_B_Spearman: 0.9145 - val_loss: 7.5865 - val_Dense_A_loss: 3.5774 - val_Dense_B_loss: 4.0090 - val_Dense_A_Spearman: 0.5905 - val_Dense_B_Spearman: 0.6013\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - 12s 420ms/step - loss: 1.1876 - Dense_A_loss: 0.5744 - Dense_B_loss: 0.6133 - Dense_A_Spearman: 0.9242 - Dense_B_Spearman: 0.9178 - val_loss: 6.0705 - val_Dense_A_loss: 2.8963 - val_Dense_B_loss: 3.1742 - val_Dense_A_Spearman: 0.5765 - val_Dense_B_Spearman: 0.5851\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - 12s 429ms/step - loss: 1.1353 - Dense_A_loss: 0.5537 - Dense_B_loss: 0.5816 - Dense_A_Spearman: 0.9322 - Dense_B_Spearman: 0.9235 - val_loss: 7.1911 - val_Dense_A_loss: 3.4384 - val_Dense_B_loss: 3.7527 - val_Dense_A_Spearman: 0.5860 - val_Dense_B_Spearman: 0.6004\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - 12s 426ms/step - loss: 1.0612 - Dense_A_loss: 0.5158 - Dense_B_loss: 0.5454 - Dense_A_Spearman: 0.9334 - Dense_B_Spearman: 0.9265 - val_loss: 7.0299 - val_Dense_A_loss: 3.3244 - val_Dense_B_loss: 3.7055 - val_Dense_A_Spearman: 0.5886 - val_Dense_B_Spearman: 0.5964\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - 12s 418ms/step - loss: 1.0259 - Dense_A_loss: 0.5004 - Dense_B_loss: 0.5255 - Dense_A_Spearman: 0.9371 - Dense_B_Spearman: 0.9342 - val_loss: 6.4990 - val_Dense_A_loss: 3.1422 - val_Dense_B_loss: 3.3569 - val_Dense_A_Spearman: 0.5786 - val_Dense_B_Spearman: 0.5879\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - 12s 412ms/step - loss: 0.9402 - Dense_A_loss: 0.4615 - Dense_B_loss: 0.4787 - Dense_A_Spearman: 0.9401 - Dense_B_Spearman: 0.9372 - val_loss: 5.9729 - val_Dense_A_loss: 2.9344 - val_Dense_B_loss: 3.0385 - val_Dense_A_Spearman: 0.5775 - val_Dense_B_Spearman: 0.5848\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - 12s 423ms/step - loss: 0.9153 - Dense_A_loss: 0.4407 - Dense_B_loss: 0.4746 - Dense_A_Spearman: 0.9425 - Dense_B_Spearman: 0.9384 - val_loss: 5.7763 - val_Dense_A_loss: 2.8189 - val_Dense_B_loss: 2.9573 - val_Dense_A_Spearman: 0.5736 - val_Dense_B_Spearman: 0.5909\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - 12s 423ms/step - loss: 0.8579 - Dense_A_loss: 0.4318 - Dense_B_loss: 0.4261 - Dense_A_Spearman: 0.9479 - Dense_B_Spearman: 0.9452 - val_loss: 5.6974 - val_Dense_A_loss: 2.7966 - val_Dense_B_loss: 2.9008 - val_Dense_A_Spearman: 0.5871 - val_Dense_B_Spearman: 0.5961\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - 12s 428ms/step - loss: 0.8595 - Dense_A_loss: 0.4176 - Dense_B_loss: 0.4419 - Dense_A_Spearman: 0.9466 - Dense_B_Spearman: 0.9412 - val_loss: 5.7749 - val_Dense_A_loss: 2.8669 - val_Dense_B_loss: 2.9079 - val_Dense_A_Spearman: 0.5736 - val_Dense_B_Spearman: 0.5913\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - 12s 419ms/step - loss: 0.7922 - Dense_A_loss: 0.3940 - Dense_B_loss: 0.3982 - Dense_A_Spearman: 0.9504 - Dense_B_Spearman: 0.9471 - val_loss: 5.7136 - val_Dense_A_loss: 2.7915 - val_Dense_B_loss: 2.9221 - val_Dense_A_Spearman: 0.5862 - val_Dense_B_Spearman: 0.5923\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - 12s 423ms/step - loss: 0.8497 - Dense_A_loss: 0.4187 - Dense_B_loss: 0.4310 - Dense_A_Spearman: 0.9500 - Dense_B_Spearman: 0.9464 - val_loss: 5.6321 - val_Dense_A_loss: 2.7840 - val_Dense_B_loss: 2.8481 - val_Dense_A_Spearman: 0.5748 - val_Dense_B_Spearman: 0.5839\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - 12s 432ms/step - loss: 0.7714 - Dense_A_loss: 0.3699 - Dense_B_loss: 0.4016 - Dense_A_Spearman: 0.9524 - Dense_B_Spearman: 0.9490 - val_loss: 5.6470 - val_Dense_A_loss: 2.8195 - val_Dense_B_loss: 2.8275 - val_Dense_A_Spearman: 0.5771 - val_Dense_B_Spearman: 0.5934\n",
      "Epoch 29/100\n",
      "28/28 [==============================] - 12s 417ms/step - loss: 0.7328 - Dense_A_loss: 0.3582 - Dense_B_loss: 0.3747 - Dense_A_Spearman: 0.9571 - Dense_B_Spearman: 0.9543 - val_loss: 5.6889 - val_Dense_A_loss: 2.8031 - val_Dense_B_loss: 2.8858 - val_Dense_A_Spearman: 0.5793 - val_Dense_B_Spearman: 0.5886\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - 12s 418ms/step - loss: 0.7373 - Dense_A_loss: 0.3487 - Dense_B_loss: 0.3886 - Dense_A_Spearman: 0.9577 - Dense_B_Spearman: 0.9516 - val_loss: 5.7785 - val_Dense_A_loss: 2.9183 - val_Dense_B_loss: 2.8602 - val_Dense_A_Spearman: 0.5816 - val_Dense_B_Spearman: 0.5986\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - 12s 423ms/step - loss: 0.7427 - Dense_A_loss: 0.3737 - Dense_B_loss: 0.3690 - Dense_A_Spearman: 0.9575 - Dense_B_Spearman: 0.9575 - val_loss: 6.1719 - val_Dense_A_loss: 3.0609 - val_Dense_B_loss: 3.1110 - val_Dense_A_Spearman: 0.5633 - val_Dense_B_Spearman: 0.5816\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - 12s 423ms/step - loss: 0.6903 - Dense_A_loss: 0.3395 - Dense_B_loss: 0.3508 - Dense_A_Spearman: 0.9596 - Dense_B_Spearman: 0.9579 - val_loss: 6.2813 - val_Dense_A_loss: 3.0717 - val_Dense_B_loss: 3.2096 - val_Dense_A_Spearman: 0.5694 - val_Dense_B_Spearman: 0.5887\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - 12s 416ms/step - loss: 0.6699 - Dense_A_loss: 0.3206 - Dense_B_loss: 0.3493 - Dense_A_Spearman: 0.9598 - Dense_B_Spearman: 0.9565 - val_loss: 5.8324 - val_Dense_A_loss: 2.8934 - val_Dense_B_loss: 2.9390 - val_Dense_A_Spearman: 0.5639 - val_Dense_B_Spearman: 0.5820\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - 12s 415ms/step - loss: 0.6211 - Dense_A_loss: 0.3027 - Dense_B_loss: 0.3183 - Dense_A_Spearman: 0.9630 - Dense_B_Spearman: 0.9612 - val_loss: 5.8057 - val_Dense_A_loss: 2.8967 - val_Dense_B_loss: 2.9090 - val_Dense_A_Spearman: 0.5700 - val_Dense_B_Spearman: 0.5833\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - 12s 418ms/step - loss: 0.6314 - Dense_A_loss: 0.3110 - Dense_B_loss: 0.3204 - Dense_A_Spearman: 0.9640 - Dense_B_Spearman: 0.9623 - val_loss: 5.6876 - val_Dense_A_loss: 2.8095 - val_Dense_B_loss: 2.8782 - val_Dense_A_Spearman: 0.5698 - val_Dense_B_Spearman: 0.5866\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - 12s 416ms/step - loss: 0.5604 - Dense_A_loss: 0.2710 - Dense_B_loss: 0.2895 - Dense_A_Spearman: 0.9657 - Dense_B_Spearman: 0.9642 - val_loss: 5.8321 - val_Dense_A_loss: 2.9125 - val_Dense_B_loss: 2.9197 - val_Dense_A_Spearman: 0.5762 - val_Dense_B_Spearman: 0.5886\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - 12s 413ms/step - loss: 0.5983 - Dense_A_loss: 0.3036 - Dense_B_loss: 0.2947 - Dense_A_Spearman: 0.9654 - Dense_B_Spearman: 0.9656 - val_loss: 5.7919 - val_Dense_A_loss: 2.8755 - val_Dense_B_loss: 2.9164 - val_Dense_A_Spearman: 0.5625 - val_Dense_B_Spearman: 0.5841\n"
     ]
    }
   ],
   "source": [
    "#Training \n",
    "\n",
    "def train(selected_model, X_train, Y_train, X_valid, Y_valid, params):\n",
    "\n",
    "    my_history=selected_model.fit(X_train, Y_train,\n",
    "                                  validation_data=(X_valid, Y_valid),\n",
    "                                  batch_size=params['batch_size'], epochs=params['epochs'],\n",
    "                                  callbacks=[EarlyStopping(patience=params['early_stop'], monitor=\"val_loss\", restore_best_weights=True),\n",
    "                                             History()])\n",
    "    \n",
    "    return selected_model, my_history\n",
    "\n",
    "\n",
    "main_model, main_params = SynIgP()\n",
    "main_model, my_history = train(main_model, X_train, Y_train, X_valid, Y_valid, main_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 2s 66ms/step\n",
      "train MSE A = 0.23\n",
      "train PCC A = 0.98\n",
      "train SCC A = 0.98\n",
      "12/12 [==============================] - 1s 65ms/step\n",
      "validation MSE A = 2.78\n",
      "validation PCC A = 0.56\n",
      "validation SCC A = 0.58\n",
      "4/4 [==============================] - 0s 64ms/step\n",
      "test MSE A = 2.01\n",
      "test PCC A = 0.65\n",
      "test SCC A = 0.66\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the Model\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create functions\n",
    "def summary_statistics(X, Y, set, task):\n",
    "    pred = main_model.predict(X, batch_size=main_params['batch_size'])\n",
    "    if task ==\"A\":\n",
    "        i=0\n",
    "    if task ==\"B\":\n",
    "        i=1\n",
    "    print(set + ' MSE ' + task + ' = ' + str(\"{0:0.2f}\".format(mean_squared_error(Y, pred[i].squeeze()))))\n",
    "    print(set + ' PCC ' + task + ' = ' + str(\"{0:0.2f}\".format(stats.pearsonr(Y, pred[i].squeeze())[0])))\n",
    "    print(set + ' SCC ' + task + ' = ' + str(\"{0:0.2f}\".format(stats.spearmanr(Y, pred[i].squeeze())[0])))\n",
    "    \n",
    "# run for each set and enhancer type\n",
    "summary_statistics(X_train, Y_train[0], \"train\", \"A\")\n",
    "summary_statistics(X_valid, Y_valid[0], \"validation\", \"A\")\n",
    "summary_statistics(X_test, Y_test[0], \"test\", \"A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "#Save model weights\n",
    "\n",
    "model_name=\"SynIgP\"\n",
    "\n",
    "model_json = main_model.to_json()\n",
    "with open('Model_' + model_name + '.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "main_model.save_weights('Model_' + model_name + '.h5')\n",
    "\n",
    "#save entire model\n",
    "main_model.save('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-aa151cbd3d32>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-aa151cbd3d32>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    seq = # 248 bp DNA sequences\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# model predict\n",
    "\n",
    "seq = # 248 bp DNA sequences\n",
    "seq = np.array(seq)\n",
    "seq_matrix = SequenceHelper.do_one_hot_encoding(seq, 248, SequenceHelper.parse_alpha_to_seq)\n",
    "\n",
    "model_ID = 'SynIgP'\n",
    "\n",
    "### load model\n",
    "def load_model(model_path):\n",
    "    import deeplift\n",
    "    from keras.models import model_from_json\n",
    "    keras_model_weights = model_path + '.h5'\n",
    "    keras_model_json = model_path + '.json'\n",
    "    keras_model = model_from_json(open(keras_model_json).read())\n",
    "    keras_model.load_weights(keras_model_weights)\n",
    "    #keras_model.summary()\n",
    "    return keras_model, keras_model_weights, keras_model_json\n",
    "\n",
    "keras_model, keras_model_weights, keras_model_json = load_model(model_ID)\n",
    "\n",
    "\n",
    "pred=keras_model.predict(seq_matrix)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
